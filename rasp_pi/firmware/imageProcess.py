from PIL import Image
#from picamera import PiCamera
import pytesseract
from imutils.object_detection import non_max_suppression
import numpy as np
import tensorflow as tf
from tensorflow import keras
import cv2
import os

# from time import sleep

#camera = PiCamera()
path = 'image.jpg'
foilModelh5 = 'foilModel.h5'
setIconModelh5 = 'setIconModel.h5'
east = 'frozen_east_text_detection.pb'
test = 'test.jpg'

'''
Takes a picture and saves it in the specified "path" (global variable)
Commented out parts allow you to view the image on a display, then take
the picture by pressing a key.

PARAMETERS
none

RETURNS
none
'''
def takePicture():
	#camera.start_preview()
	#input()
	camera.resolution = (2592, 1944)
	camera.capture(path)
	#print("took a pic")
	#camera.stop_preview()
	#sleep(2)

'''
Rotates and crops the image to the general section that we care about.
More processing may be needed by the different functions to the specific
area that we want.

PARAMETERS
none

RETURNS
none
'''
def preProcessImage():
	image = cv2.imread(path)
	rotated_img = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
	out = rotated_img[550:1700, 20:1450]
	cv2.imwrite(path, out)

'''
Produces the prediction values based on the trained model. Detects whether
a card is foiled and the set it belongs to (requires separate calls)

PARAMETERS
model: The tensorflow model being used
image: The cv2 image being processed

RETURNS
The most likely set that the card belongs to, or whether it's foiled
'''
def recognizeObject(model, image):
    img = (np.expand_dims(image, 0))
    if img.shape[1:] != model.input_shape[1:]:
        # Input image is not the same size as the desired input
        error = 'Got input shape: ' +str(img.shape[1:]) + ', expected shape: ' + str(model.input_shape[1:])
        return error

    predictions = model.predict(img)
    return predictions

'''
Determines whether or not the card is foiled based on the predictions
generated by the model

PARAMETERS
none

RETURNS
Whether or not the card is foiled

'''
def foilRecognition():
	# Load model and image, extract HSV channel and send in the desired
	# input shape
	foilModel = keras.models.load_model(foilModelh5)

	testImg = cv2.imread(path)
	testImg = cv2.cvtColor(testImg, cv2.COLOR_RGB2HSV)[:,:,0] 
	inputShape = foilModel.input_shape[1:]
	resized = cv2.resize(testImg, inputShape[::-1])
	predictions = recognizeObject(foilModel, resized/255)

	print(predictions)
	if (predictions[0][0] > predictions[0][1]):
		return True
	return False

'''
Compares values returned from set symbol recognition with the 
confidenceArray and decides on the most likely set symbol 

PARAMETERS
class_names: The list of sets that are mapped to the confidence array
possibleSets: The list of possible sets

RETURNS
The most likely set that the card belongs to

'''
def setRecognition(classNames, possibleSets):
	# We need to do some img processing first
	image = cv2.imread(path)
	image = image[940:1230, 1080:1310]

	rgb_planes = cv2.split(image)

	result_planes = []
	for plane in rgb_planes:
		dilated_img = cv2.dilate(plane, np.ones((7,7), np.uint8))
		bg_img = cv2.medianBlur(dilated_img, 21)
		diff_img = 255 - cv2.absdiff(plane, bg_img)
		result_planes.append(diff_img)

	result = cv2.merge(result_planes)

	gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
	gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]

	resized = cv2.resize(gray, (90, 90))
	cv2.imwrite(test, resized)

	setIconModel = keras.models.load_model(setIconModelh5)
	
	confidenceArray = recognizeObject(setIconModel, resized/255)
	confidence = 0
	likelySet = "None"

	for i in possibleSets:
		index = classNames.index(i)
		if (confidenceArray[0][index] > confidence):
			likelySet = i
			confidence = confidenceArray[0][index]

	likelySet = likelySet.replace("_", " ")
	return likelySet

'''
Used for text_recognition. Processes the predictions scores that we got
and returns their respective confidences

PARAMETERS
scores: Scores associated with the boxes
geometry: Current coordinates

RETURNS
A tuple of the bounding boxes and their associated confidences

'''
def decode_predictions(scores, geometry):
	# grab the number of rows and columns from the scores volume, then
	# initialize our set of bounding box rectangles and corresponding
	# confidence scores
	(numRows, numCols) = scores.shape[2:4]
	rects = []
	confidences = []
 
	# loop over the number of rows
	for y in range(0, numRows):
		# extract the scores (probabilities), followed by the
		# geometrical data used to derive potential bounding box
		# coordinates that surround text
		scoresData = scores[0, 0, y]
		xData0 = geometry[0, 0, y]
		xData1 = geometry[0, 1, y]
		xData2 = geometry[0, 2, y]
		xData3 = geometry[0, 3, y]
		anglesData = geometry[0, 4, y]
 
		# loop over the number of columns
		for x in range(0, numCols):
			# if our score does not have sufficient probability,
			# ignore it
			if scoresData[x] < 0.5:
				continue
 
			# compute the offset factor as our resulting feature
			# maps will be 4x smaller than the input image
			(offsetX, offsetY) = (x * 4.0, y * 4.0)
 
			# extract the rotation angle for the prediction and
			# then compute the sin and cosine
			angle = anglesData[x]
			cos = np.cos(angle)
			sin = np.sin(angle)
 
			# use the geometry volume to derive the width and height
			# of the bounding box
			h = xData0[x] + xData2[x]
			w = xData1[x] + xData3[x]
 
			# compute both the starting and ending (x, y)-coordinates
			# for the text prediction bounding box
			endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
			endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
			startX = int(endX - w)
			startY = int(endY - h)
 
			# add the bounding box coordinates and probability score
			# to our respective lists
			rects.append((startX, startY, endX, endY))
			confidences.append(scoresData[x])
 
	# return a tuple of the bounding boxes and associated confidences
	return (rects, confidences)

'''
Processes the image taken and uses PyTesseract to extract the card name

PARAMETERS
none

RETURNS
The text extracted from the card (the name)
'''
def textRecognition():
	# load the input image and grab the image dimensions
	image = cv2.imread(path)
	image = image[0:900, 0:1100]
	cv2.imwrite('text.jpg', image)
	orig = image.copy()
	(origH, origW) = image.shape[:2]
	 
	# set the new width and height and then determine the ratio in change
	# for both the width and height
	(newW, newH) = (320, 320)
	rW = origW / float(newW)
	rH = origH / float(newH)
	 
	# resize the image and grab the new image dimensions
	image = cv2.resize(image, (newW, newH))
	(H, W) = image.shape[:2]

	# define the two output layer names for the EAST detector model that
	# we are interested in -- the first is the output probabilities and the
	# second can be used to derive the bounding box coordinates of text
	layerNames = [
		"feature_fusion/Conv_7/Sigmoid",
		"feature_fusion/concat_3"]
	 
	# load the pre-trained EAST text detector
	net = cv2.dnn.readNet(east)

	# construct a blob from the image and then perform a forward pass of
	# the model to obtain the two output layer sets
	blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),
		(123.68, 116.78, 103.94), swapRB=True, crop=False)
	net.setInput(blob)
	(scores, geometry) = net.forward(layerNames)
	 
	# decode the predictions, then  apply non-maxima suppression to
	# suppress weak, overlapping bounding boxes
	(rects, confidences) = decode_predictions(scores, geometry)
	boxes = non_max_suppression(np.array(rects), probs=confidences)

	# initialize the list of results
	results = []
	 
	# loop over the bounding boxes
	for (startX, startY, endX, endY) in boxes:
		# scale the bounding box coordinates based on the respective ratios
		startX = int(startX * rW)
		startY = int(startY * rH)
		endX = int(endX * rW)
		endY = int(endY * rH)
	 
		# in order to obtain a better OCR of the text we can potentially
		# apply a bit of padding surrounding the bounding box 
		dX = int((endX - startX) * 0.03)
		dY = int((endY - startY) * 0.12)
	 
		# apply padding to each side of the bounding box, respectively
		startX = max(0, startX - dX)
		startY = max(0, startY - dY)
		endX = min(origW, endX + (dX * 2))
		endY = min(origH, endY + (dY * 2))
	 
		results.append((startX, startY, endX, endY))

	# sort the results bounding box coordinates from left to right
	results = sorted(results, key=lambda r:r[0])

	# get the predicted size of the entire text, based on the separate
	# box bounds of the individual words
	numObj = len(results)-1
	startX = results[0][0]
	endX = results[numObj][2]
	startY = min(results[numObj][1], results[0][1])
	endY = max(results[numObj][3], results[0][3])

	# Sometimes it'll detect something super far away 
	while(((endY - startY) > 100) and (numObj >= 0)):
		endY = max(results[numObj][3], results[0][3])
		numObj = numObj - 1

	roi = orig[startY:endY, startX:endX]
	roi = cv2.blur(roi,(5,5))
	cv2.imwrite(test, roi)

	# in order to apply Tesseract v4 to OCR text we must supply
	# (1) a language, (2) an OEM flag of 1, indicating that the we
	# wish to use the LSTM neural net model for OCR, and finally
	# (3) an OEM value, in this case, 7 which implies that we are
	# treating the ROI as a single line of text
	config = ("-l eng --oem 1 --psm 7")
	text = pytesseract.image_to_string(roi, config=config)
	
	if(text):
		# Common fixes for stuff
		if not(text[0].isalpha()):
			text = text[1:]
		if not(text[-1].isalpha()):
			text = text[:-1]
		if not(text[-1].isalpha()):
			text = text[:-1]
		text = text.replace("â€™", "'")
		text = text.replace(".", ",")

	#os.remove("text.jpg")

	return text